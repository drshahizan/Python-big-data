{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drshahizan/Python-big-data/blob/main/assignment/ass6/bdm/Kicap/big_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUTye1zKGXv1"
      },
      "source": [
        "#**Assignment 6: Mastering Big Data Handling**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uITTSCstesAb"
      },
      "source": [
        "\n",
        "**TEAM MEMBERS:**\n",
        "```\n",
        "NABILA HUSNA BINTI ROSLI (MCS231009)\n",
        "NUR AZIMAH BINTI MOHD SALLEH (MCS231011)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-CZ6TL1eugC"
      },
      "source": [
        "##**Pick a Big Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlJQvrfad73M"
      },
      "source": [
        "###**Dataset :** `Restaurant reviews`\n",
        "\n",
        "\n",
        "###**About**\n",
        "This dataset, labeled \"Restaurant reviews,\" which is a collection of information and the feedback from customers about the restaurant.\n",
        "\n",
        "The dataset was divided into 2 parts which are the reviews during pre-covid and post-covid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW9E6PoDe9fo"
      },
      "source": [
        "##**Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "aOvWaS33GSCx",
        "outputId": "88a519fd-e498-431b-b374-a81826fd5da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c045510-a320-4b94-8681-b4d7da6fe4b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c045510-a320-4b94-8681-b4d7da6fe4b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Upload kaggle.json API token, and download / unzip the restaurant-reviews zip file dataset\n",
        "\n",
        "# Install and upload the kaggle.json file\n",
        "!pip install kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f32UaleOiWP",
        "outputId": "5de3d2a5-c247-4ecf-99c7-1c807f71436f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading restaurant-reviews.zip to /content\n",
            " 99% 2.02G/2.04G [00:29<00:00, 76.2MB/s]\n",
            "100% 2.04G/2.04G [00:30<00:00, 72.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d fahadsyed97/restaurant-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl4Umsyh0Vw",
        "outputId": "4d6197e4-a237-4fba-854d-b28638a298e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  restaurant-reviews.zip\n",
            "  inflating: postcovid_reviews.csv   \n",
            "  inflating: precovid_reviews.csv    \n"
          ]
        }
      ],
      "source": [
        "!unzip restaurant-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3GW44bPvL7vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Combine all csv into 1 csv and mount it in GoogleDrive**"
      ],
      "metadata": {
        "id": "9eguAQCwP6aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 files that need to be merged - both having the same columns\n",
        "file_paths = [\n",
        "    '/content/postcovid_reviews.csv',\n",
        "    '/content/precovid_reviews.csv'\n",
        "    ]\n",
        "\n",
        "# Create an empty list to store DataFrames\n",
        "dataframes = []"
      ],
      "metadata": {
        "id": "vWuH4N75L2vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read each CSV file into a DataFrame and append to the list\n",
        "for file_path in file_paths:\n",
        "    d_frame = pd.read_csv(file_path)\n",
        "    dataframes.append(d_frame)"
      ],
      "metadata": {
        "id": "6_rnsDM1L3i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all DataFrames into one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "Ko-wOjwUL-a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('restaurant-reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "DyVYwaKUPIkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.info(verbose=False, memory_usage='deep')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPoOTjhRMzSY",
        "outputId": "7f1a004e-c7e4-4b7a-8f34-06042b7e84a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572493 entries, 0 to 5572492\n",
            "Columns: 21 entries, business_id to date_\n",
            "dtypes: float64(3), int64(6), object(12)\n",
            "memory usage: 8.8 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uOwbYiSgiHa"
      },
      "source": [
        "##**Strategies for Big Datasets**\n",
        "\n",
        "\n",
        "\n",
        "*   Using merged dataset ('`restaurant-reviews.csv`')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcOF5N9dvXA"
      },
      "source": [
        "### **Load Less Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By loading only the first 1000 rows (nrows=1000), we've created a DataFrame (df) that represents a subset of the original data."
      ],
      "metadata": {
        "id": "CUhwuaaziRiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('restaurant-reviews.csv', nrows=1000)\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OabIsjxaOlR4",
        "outputId": "aa6f7661-67d4-4a16-f449-1796d42a7ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "business_id        object\n",
              "name               object\n",
              "address            object\n",
              "state_             object\n",
              "city               object\n",
              "postal_code        object\n",
              "latitude          float64\n",
              "longitude         float64\n",
              "stars             float64\n",
              "review_count        int64\n",
              "is_open             int64\n",
              "categories         object\n",
              "hours              object\n",
              "review_id          object\n",
              "user_id            object\n",
              "customer_stars      int64\n",
              "useful              int64\n",
              "funny               int64\n",
              "cool                int64\n",
              "text_              object\n",
              "date_              object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data types of the columns have been automatically inferred by pandas during the reading process. For example, columns like 'business_id' and 'name' are stored as object (text) types, while numerical columns like 'latitude', 'longitude', 'stars', 'review_count', and others are stored as float64 or int64."
      ],
      "metadata": {
        "id": "2ayXoopCiVFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.memory_usage().sum()/(1024*1024*1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k3WY3qKOiHu",
        "outputId": "fe07db4d-8ae7-45a0-8df8-9f6d992d9f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00015658140182495117"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The memory usage of this subset DataFrame is relatively small, approximately 0.0001565814 GB. This is because you loaded only a fraction of the original data, which can be beneficial for quick exploration and analysis when you don't need the entire dataset."
      ],
      "metadata": {
        "id": "wMbAQvZ-iX0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3pwMrsDMVJmA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m74kNPWXdyHU"
      },
      "source": [
        "### **Use Chunking**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code reads the entire CSV file into a Pandas DataFrame without using chunks and measures the time it takes to complete."
      ],
      "metadata": {
        "id": "uEuPJrJ-hCGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the Entire DataFrame Without Using Chunks\n",
        "%%time\n",
        "df = pd.read_csv('restaurant-reviews.csv')\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bh-PkYDXw8d",
        "outputId": "bfb2a48a-43aa-40d8-f3d5-db915b2c938c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 56.3 s, sys: 4.93 s, total: 1min 1s\n",
            "Wall time: 1min 4s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5572493"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows that reading the entire file took approximately 1 minute and 4 seconds, and the DataFrame has 5,572,493 entries."
      ],
      "metadata": {
        "id": "DUg_D1eAhEuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code reads the CSV file using chunks of size 1000 rows and measures the time it takes to initialize the chunked reader."
      ],
      "metadata": {
        "id": "QTUuTu-GhGY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the CSV File Using Chunks\n",
        "%%time\n",
        "chunks = pd.read_csv('restaurant-reviews.csv', iterator=True, chunksize=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZgTxSvKOthB",
        "outputId": "ed04cf8a-62ec-4ba5-c4b6-82f32ebb6d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.22 ms, sys: 1.07 ms, total: 3.29 ms\n",
            "Wall time: 5.06 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows that setting up the chunked reader took a very short time, much less than a second."
      ],
      "metadata": {
        "id": "MDFLSICqhHGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code iterates through each chunk of the file and calculates the total length by summing up the lengths of individual chunks. It also measures the time it takes to complete."
      ],
      "metadata": {
        "id": "WNk0NU-ThNeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating Through Chunks and Calculating Total Length\n",
        "length = 0\n",
        "for chunk in chunks:\n",
        "    length += len(chunk)\n",
        "length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4efhskyHOup6",
        "outputId": "ebc06c2b-034e-40aa-919e-864cf7f47f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5572493"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows that iterating through chunks and calculating the total length took approximately 1 minute and 19 seconds. The resulting length matches the total number of entries in the DataFrame."
      ],
      "metadata": {
        "id": "8rHM5XohhO7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qoArWYC5VKW-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrpWrGwsevLq"
      },
      "source": [
        "### **Optimize Data Types**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The code below is the way to optimize the memory usage of a pandas dataframe by adjusting the data types of its column (fine-tune data types)."
      ],
      "metadata": {
        "id": "p9084roFac0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints information about the DataFrame, showing the data types of each column and the memory usage."
      ],
      "metadata": {
        "id": "hYhDVF8sf3SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To have the overview of the initial dataframe information.\n",
        "df.info(verbose=False, memory_usage='deep')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK-UpBOcYY5a",
        "outputId": "35c8d2fd-59e3-4891-d47e-949809f6591a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572493 entries, 0 to 5572492\n",
            "Columns: 21 entries, business_id to date_\n",
            "dtypes: float64(3), int64(6), object(12)\n",
            "memory usage: 8.8 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the DataFrame has 21 columns with data types: float64, int64, and object. The DataFrame has 5,572,493 entries and initially consumes 8.8 GB of memory."
      ],
      "metadata": {
        "id": "w9orvrA3gJuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the initial memory usage\n",
        "start_mem = df.memory_usage().sum() / 1024**3\n",
        "print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eok5QhSYktG",
        "outputId": "a7a23a07-0b00-4fb5-8cd3-15a1285fbdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage of dataframe is 0.87 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial memory usage before any optimization is 0.87 MB"
      ],
      "metadata": {
        "id": "Vtli3__EgWWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This loop iterates through each column of the DataFrame and checks its data type. If the data type is float64, it converts it to float16. If it's int64, it converts it to int16. If it's an object, it converts it to a categorical data type."
      ],
      "metadata": {
        "id": "bbyP-rUcgigV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Type Optimization Loop\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'float64':\n",
        "        df[col] = df[col].astype('float16')\n",
        "    if df[col].dtype == 'int64':\n",
        "        df[col] = df[col].astype('int16')\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].astype('category')"
      ],
      "metadata": {
        "id": "GxfwdkfnZEUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This calculates and prints the memory usage of the DataFrame after the data type optimization and the percentage decrease in memory usage."
      ],
      "metadata": {
        "id": "jlRpgzrZghES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_mem = df.memory_usage().sum() / 1024**3\n",
        "print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OWcsfmNZqt5",
        "outputId": "d2bbce71-9161-48b1-b504-6c63f2ceb605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after optimization is: 0.84 MB\n",
            "Decreased by 4.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the optimization, the memory usage has been reduced from 8.8 GB to 4.9 GB, and the percentage decrease is 4.0%."
      ],
      "metadata": {
        "id": "atuU5QVNgmNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the final dataframe information\n",
        "df.info(verbose=False, memory_usage='deep')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeXSi2IeZzIb",
        "outputId": "90b1bd4f-25a4-46fa-9578-f341142a23e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572493 entries, 0 to 5572492\n",
            "Columns: 21 entries, business_id to date_\n",
            "dtypes: category(12), float16(3), int16(6)\n",
            "memory usage: 4.9 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final DataFrame has the same number of entries but uses less memory, with optimized data types: category(12), float16(3), and int16(6).\n",
        "\n",
        "It has been reduced from 8.8 GB to 4.9 GB after implemented the optimization of data types."
      ],
      "metadata": {
        "id": "UVYA2cjvZ8Kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of data types optimization: -\n",
        "\n",
        "* The initial DataFrame had a memory usage of 8.8 GB.\n",
        "* After optimizing the data types (using float16, int16, and category), the memory usage decreased to 4.9 GB.\n",
        "* The optimization resulted in a 4.0% reduction in memory usage."
      ],
      "metadata": {
        "id": "ejLSzl7Xhpo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9Bs79epwVLw6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_f1i0fPevCM"
      },
      "source": [
        "### **Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original dataframe information (5M+ rows, 21 columns)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z0hh2MijANa",
        "outputId": "d1130d67-6f69-45a4-c5e4-8c2c7829bad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572493, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet prints the shape of the original DataFrame (df), indicating that it has 5,572,493 rows and 21 columns."
      ],
      "metadata": {
        "id": "IOSkUgYVjQnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the desired sample size as 1000. This is the number of rows we want to randomly select from the original DataFrame."
      ],
      "metadata": {
        "id": "RKuoVLy1ja3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the desired sample size\n",
        "sample_size = 1000"
      ],
      "metadata": {
        "id": "_ApofG7yLqeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the sample method, you randomly select 1000 rows from the original DataFrame (df). The random_state=42 parameter ensures reproducibility, meaning the same random rows will be selected if you run the code again with the same random state."
      ],
      "metadata": {
        "id": "0lsRaqjFjh-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly sample data\n",
        "sampled_df = df.sample(n=sample_size, random_state=42)"
      ],
      "metadata": {
        "id": "-ZI50mcvVO8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints information about the sampled DataFrame, including data types and memory usage, and displays a preview of a few rows from the sampled DataFrame."
      ],
      "metadata": {
        "id": "waSGrKZ0jm10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the sampled DataFrame\n",
        "print(\"Info about Sampled DataFrame:\")\n",
        "print(sampled_df.info())\n",
        "\n",
        "# Display a few rows of the sampled DataFrame\n",
        "print(\"\\nSampled DataFrame Preview:\")\n",
        "print(sampled_df.head())"
      ],
      "metadata": {
        "id": "wEG1EHTgVOz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet prints the shape of the sampled DataFrame, confirming that it now has 1000 rows and 21 columns."
      ],
      "metadata": {
        "id": "J1UriLWijpqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jekf6ofNjIO4",
        "outputId": "a68113d2-1ebe-4d59-a63f-f20cd49635b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary using sampling: -\n",
        "\n",
        "\n",
        "*   The original DataFrame (df) has 5,572,493 rows and 21 columns.\n",
        "You randomly sampled 1000 rows from the original\n",
        "* DataFrame to create a smaller subset, resulting in a new DataFrame (sampled_df) with the same number of columns but only 1000 rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "3OyhH2dxjsUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "py26QZRpVMgd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUCZqhxVeu0L"
      },
      "source": [
        "## **Parallelize with Dask**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dask is a powerful library for parallel and distributed computing. It allows us to scale our computations by parallelizing them across multiple cores or even distributed computing clusters."
      ],
      "metadata": {
        "id": "-Jsn9VUpkJ_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ddf` is the Dask DataFrame created from the original pandas DataFrame df. The `npartitions` parameter determines how the DataFrame is divided for parallel processing."
      ],
      "metadata": {
        "id": "vOGTrC5ck-N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dask library\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Convert the pandas DataFrame to a Dask DataFrame\n",
        "ddf = dd.from_pandas(df, npartitions=4)"
      ],
      "metadata": {
        "id": "j0L8sAVxVNx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The operation to calculate the mean of the 'stars' column is performed using Dask's lazy evaluation. The actual computation is triggered by calling compute()."
      ],
      "metadata": {
        "id": "3jddjhyRlJPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are all the code that perform the operations in parallel."
      ],
      "metadata": {
        "id": "av4mOuAGooZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform some operations in parallel\n",
        "# Example 1: Calculate the mean of the 'stars' column\n",
        "mean_stars = ddf['stars'].mean()\n",
        "\n",
        "# Compute the result\n",
        "result = mean_stars.compute()\n",
        "\n",
        "# Display the result\n",
        "print(\"Mean Stars:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inMnxesrLq5b",
        "outputId": "c9247c76-6681-44bd-b4bf-6f8d3b73193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Stars: 4.044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szjk0QDWg2MO"
      },
      "outputs": [],
      "source": [
        "# Example 2: Filtering data in parallel\n",
        "filtered_data = ddf[ddf['stars'] > 4]\n",
        "\n",
        "# Compute the results\n",
        "filtered_data_result = filtered_data.compute()\n",
        "\n",
        "# Display the results\n",
        "print(\"Filtered Data:\")\n",
        "print(filtered_data_result.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Groupby and compute mean in parallel\n",
        "mean_stars_by_city = ddf.groupby('city')['stars'].mean()\n",
        "\n",
        "# Compute the results\n",
        "mean_stars_by_city_result = mean_stars_by_city.compute()\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nMean Stars by City:\")\n",
        "print(mean_stars_by_city_result.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UAK7Ln7n7qJ",
        "outputId": "47c49b38-d694-49c8-d10d-26f7e4317a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Stars by City:\n",
            "city\n",
            "Allston      4.041667\n",
            "Atlanta      3.915966\n",
            "Austin       4.269355\n",
            "Avon         2.500000\n",
            "Beaverton    4.321429\n",
            "Name: stars, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Count the number of reviews for each star rating in parallel\n",
        "review_counts_by_stars = ddf['stars'].value_counts()\n",
        "\n",
        "# Compute the results\n",
        "review_counts_by_stars_result = review_counts_by_stars.compute()\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nReview Counts by Stars:\")\n",
        "print(review_counts_by_stars_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GKJU9rOnjFc",
        "outputId": "987602c2-7fa2-40ab-d8fd-7f5df038c7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Review Counts by Stars:\n",
            "4.0    399\n",
            "4.5    387\n",
            "3.5    106\n",
            "3.0     56\n",
            "5.0     19\n",
            "2.5     18\n",
            "2.0     10\n",
            "1.5      5\n",
            "Name: stars, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary for parallelize using Dask: -\n",
        "* Perform filtering, groupby, and value counts on the Dask DataFrame (ddf), and all these operations are executed in parallel.\n",
        "* The compute() method is used to trigger the actual computation and obtain the results.\n",
        "* The optional Dask Client setup is included. If you have a distributed computing environment, the Dask Client can be used to manage computations across multiple workers."
      ],
      "metadata": {
        "id": "LW_SDbbFoVze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DxlgZtLNVNTs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F40R2qPUgpLw"
      },
      "source": [
        "##**Comparative Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros and cons for both traditional method and advanced strategies used in Assignment 6: -\n",
        "\n",
        "###**Pandas**\n",
        "* Pros\n",
        "1. Simplicity and ease of use.\n",
        "2. Suitable for smaller datasets that fit into memory\n",
        "3. Operations are performed sequentially on a single node.\n",
        "4. File size is determined by the size of the dataset and the data types used.\n",
        "\n",
        "* Cons\n",
        "1. Loads the entire dataset into memory, which can be limiting for large datasets.\n",
        "2. May lead to MemoryError for very large datasets.\n",
        "3. Computation time increases linearly with the size of the dataset.\n",
        "4. Limited parallelization.\n",
        "\n",
        "\n",
        "###**Dask**\n",
        "* Pros\n",
        "1. Operates on larger-than-memory datasets by dividing them into smaller partitions.\n",
        "2. More scalable and adaptive memory usage.\n",
        "3. Lazy evaluation minimizes unnecessary loading of data.\n",
        "4. Parallelizes operations by dividing the dataset into partitions.\n",
        "5. Significantly reduces computation time, especially for parallelizable operations.\n",
        "6. Efficiently handles larger datasets without necessarily increasing file size.\n",
        "7. Lazy evaluation minimizes the need to load the entire dataset into memory.\n",
        "\n",
        "* Cons\n",
        "1. Requires careful consideration of the partitioning strategy.\n",
        "2. Overheads associated with task scheduling and communication.\n",
        "\n",
        "\n",
        "\n",
        "Below are the comparison analysis between pandas (traditional methods) and Dask (advanced strategies). By comparing in a few terms: -\n",
        "\n",
        "* ### Memory usage\n",
        "  \n",
        "  Advanced strategies (Dask) have a clear advantage for handling larger-than-memory datasets with more efficient memory usage.\n",
        "\n",
        "* ### Computation time\n",
        "  Advanced strategies (Dask) show a significant advantage, especially when operations can be parallelized.\n",
        "\n",
        "* ### File size\n",
        "  Advanced strategies (Dask) are advantageous for efficiently handling larger datasets without a proportional increase in file size."
      ],
      "metadata": {
        "id": "rKVz4b98o03n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LADg_H0Noxm-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dAXL7wUgppw"
      },
      "source": [
        "##**Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading less data** is a common strategy when working with large datasets, allowing you to save memory and speed up initial data exploration tasks. However, keep in mind that working with a subset may not represent the full dataset, and decisions based on this subset should be made with caution."
      ],
      "metadata": {
        "id": "wXbR8ND-iLYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary benefit of **optimizing data types** is to reduce memory usage, making it more efficient for handling and processing large datasets. It's important to choose the appropriate data types for columns to balance memory efficiency with the precision needed for analysis."
      ],
      "metadata": {
        "id": "MYh4YBKWhfFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using **chunks** is beneficial when dealing with large datasets that may not fit into memory. Instead of loading the entire dataset at once, it is read in smaller chunks. This allows for more efficient memory usage and processing. In the example, the time to read the entire dataset using chunks might be longer than reading it without chunks. However, the benefit becomes apparent when the dataset is too large to fit into memory, as chunks allow you to process and analyze portions of the data at a time."
      ],
      "metadata": {
        "id": "YayfUZychXTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling** is a useful technique for working with large datasets when you want to get a representative subset for exploratory data analysis or model building. The random_state parameter is set for reproducibility, ensuring that the same random rows are selected if the code is run again with the same random state."
      ],
      "metadata": {
        "id": "IXZHwtAlj1ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **After using Dask:**\n",
        "* Traditional methods (pandas) are suitable for smaller datasets but may face limitations in terms of memory and computation time for larger datasets.\n",
        "* Advanced strategies (Dask), offer scalability, efficient memory usage, and parallelization, making them highly advantageous for big data scenarios.\n",
        "* The choice between traditional methods (pandas) and advanced strategies (Dask) depends on the size and complexity of the data, available computing resources, and the specific requirements of the analysis.\n"
      ],
      "metadata": {
        "id": "-YAPI6lxps5Q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7LPJBSQOe+SxkfNUBiKeq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}