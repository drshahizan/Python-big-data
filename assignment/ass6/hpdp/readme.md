<a href="https://github.com/drshahizan/Python-big-data/stargazers"><img src="https://img.shields.io/github/stars/drshahizan/Python-big-data" alt="Stars Badge"/></a>
<a href="https://github.com/drshahizan/Python-big-data/network/members"><img src="https://img.shields.io/github/forks/drshahizan/Python-big-data" alt="Forks Badge"/></a>
<a href="https://github.com/drshahizan/Python-big-data/pulls"><img src="https://img.shields.io/github/issues-pr/drshahizan/Python-big-data" alt="Pull Requests Badge"/></a>
<a href="https://github.com/drshahizan/Python-big-data/issues"><img src="https://img.shields.io/github/issues/drshahizan/Python-big-data" alt="Issues Badge"/></a>
<a href="https://github.com/drshahizan/Python-big-data/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/drshahizan/Python-big-data?color=2b9348"></a>
![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan%2FPython-big-data&labelColor=%23d9e3f0&countColor=%23697689&style=flat)

Don't forget to hit the :star: if you like this repo.

# Assignment 6: Mastering Big Data Handling

## Introduction
In this assignment, you will explore the management of big data processing in data science. Big data processing involves the systematic handling and analysis of vast and complex datasets that exceed the capabilities of traditional data processing methods. It encompasses the storage, retrieval, and manipulation of massive volumes of information to extract valuable insights.

## Task Overview
1. **Pick a Big Dataset**: Start by choosing a suitable dataset. Choose a dataset from reputable sources such as Kaggle, UCI Machine Learning Repository, or any other pertinent dataset repository. Make sure it's big‚Äîover **700 MB**.

2. **Loading the Dataset**: Use Python and Pandas to load your chosen dataset into your Colab notebook. You can either upload it from your computer or directly from an online source.

3. **Strategies for Big Datasets**: Apply four smart strategies to handle large datasets effectively:
   - *Load Less Data*: Strategically load only the essential portions of the dataset to optimize memory usage.
   - *Use Chunking*: Process the data in smaller pieces to avoid memory issues.
   - *Optimize Data Types*: Fine-tune data types to maximize efficiency and minimize memory consumption.
   - *Sampling*: Implement sampling methodologies to extract meaningful insights from a subset of the dataset.

4. **Steps for Using These Strategies**: Clearly explain the steps for each strategy. Help us understand how to make these strategies work.

5. **Comparative Analysis**: Conduct a comprehensive comparative analysis between traditional methods and advanced strategies. Evaluate aspects such as memory usage, computation time, and file size. Provide meaningful insights into the advantages gained through the adoption of advanced strategies.

6. **Conclusion**: Summarize your findings. Explain why you chose these strategies and how they make a difference in handling big data.

## Submission
1. Create a new Markdown document in Google Colab and name it "big_data.md."

2. Ensure the Markdown document is well-organized with clear explanations, appropriate headings, and bullet points.

3. Integrate Python code snippets within the Markdown document to illustrate the implementation of strategies.

4. Attach your Colab notebook (`.ipynb` file) containing all the code, annotations, and visualizations to your submission.

5. Include your name, student ID, and the date in the Markdown document.

6. Share the Markdown document and Colab notebook with your lecturer as instructed for evaluation.

üöÄ Form project teams comprising a minimum of three and a maximum of four students. Teamwork is essential for this assignment. Please complete the Google Sheets page with your group information [**here**](https://docs.google.com/spreadsheets/d/1vLDgDAu2ai9rAOIKUfE1xUfTEvK2ikpXJ_1F-Xqtk_c/edit?pli=1#gid=2103764783). Please update your group information:

| No | Group |  File | Dataset | 
| -----: |  ------ | :-----: |  ----- |  
| 0. | Sample  |  <a href="./sample/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | 
| 1. | Pergolakan  | <a href="./PERgolakan/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [eCommerce behavior data from multi-category store](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store) |
| 2. | 3H  |  <a href="./3H/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [Anime Dataset 2023](https://www.kaggle.com/datasets/dbdmobile/myanimelist-dataset?select=final_animedataset.csv)|
| 3. | SYNA  |  <a href="https://github.com/drshahizan/Python_EDA/tree/main/assignment/ass4/hpdp/SYNA" ><img src="../../../images/answer.png" width="24px" height="24px" >|  [Age dataset: life, work, and death of 1.22M people](https://www.kaggle.com/datasets/imoore/age-dataset)| </a> | 
| 5. | ATG  |   <a href="./ATG/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> |[Airline Delay and Cancellation Data 2018](https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018?select=2018.csv)|
| 6. | DEADPOOL  | <a href="https://github.com/drshahizan/Python_EDA/blob/main/assignment/ass4/hpdp/DEADPOOL/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [Spotify_1_million_tracks](https://www.kaggle.com/datasets/amitanshjoshi/spotify-1million-tracks/data)|
| 7. | ANGKASA  | <a href="./ANGKASA/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [USA Real Estate Dataset](https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset) |
| 8. | ByteNav  | <a href="./ByteNav/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [Large Car Dataset](https://www.kaggle.com/datasets/cisautomotiveapi/large-car-dataset) |
| 9. | HANY  | <a href="./HANY/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [Climate Weather Surface of Brazil](https://www.kaggle.com/datasets/PROPPG-PPG/hourly-weather-surface-brazil-southeast-region) |
| 10. | BERUK  | <a href="./BERUK/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [Flight Data](https://www.kaggle.com/datasets/polartech/flight-data-with-1-million-or-more-records) |
| 11. | ZProMax  | <a href="./ZProMax/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [New York Bus Rides Service](https://www.kaggle.com/datasets/asimzahid/new-york-bus-rides-service?select=searches.csv) |
| 12. | CapKetereh  | <a href="./CapKetereh/readme.md" ><img src="../../../images/answer.png" width="24px" height="24px" ></a> | [COVID vaccination vs. mortality](https://www.kaggle.com/datasets/sinakaraji/covid-vaccination-vs-death) |


### 3. Academic Integrity
üö´ Uphold the highest standards of academic integrity. Any candidate suspected of cheating in the assignment will face disciplinary action, which may include suspension or expulsion from the University. Moreover, any materials or devices found to be in violation of examination rules and regulations will be confiscated.

### 4. Submission Requirements
üìù Prepare a comprehensive document that outlines the step-by-step process for creating the case study. 
The deadline for submission is **26 November 2023, at 5:00 PM**. Late submissions will not be accepted and will be disregarded.

## File and Folder Structure 

You must place your file in the submission folder. Within the [`hpdp/`](https://github.com/drshahizan/Python_EDA/edit/main/assignment/ass4/hpdp) folder, create a folder called your group. Name the default file as `readme.md`. Suggested folder structure for this project:

```html
hpdp/your_group/
‚îú‚îÄ‚îÄ üìÑ feature_eng.md
‚îî‚îÄ‚îÄ üìÑ readme.md

```
## Grading
Your assignment will be evaluated based on the following criteria:
- Dataset selection and loading
- Exploratory Data Analysis
- Feature selection and preprocessing
- Feature transformation and creation
- Clarity of explanations
- Proper documentation and comments in the code
- Correctness of the code and results

## Additional Resources
You can refer to the following resources for guidance and inspiration:
- Python libraries like Pandas, NumPy, and Matplotlib/Seaborn
- Online tutorials and documentation for feature engineering
- Books on data preprocessing and feature engineering in data science

Good luck with your assignment! If you have any questions or need help, don't hesitate to reach out to your instructor or fellow students.


## Contribution üõ†Ô∏è
Please create an [Issue](https://github.com/drshahizan/Python_EDA/issues) for any improvements, suggestions or errors in the content.

You can also contact me using [Linkedin](https://www.linkedin.com/in/drshahizan/) for any other queries or feedback.

[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan&labelColor=%23697689&countColor=%23555555&style=plastic)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdrshahizan)
![](https://hit.yhype.me/github/profile?user_id=81284918)


