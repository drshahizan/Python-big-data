{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xaLaICJfB-yS"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drshahizan/Python-big-data/blob/main/assignment/ass6/hpdp/2Big2Handle/big_data_2Big2Handle_md.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spotify Charts (Assignment 6: Mastering Big Data Handling)\n",
        "\n",
        "Source credit: https://www.kaggle.com/datasets/dhruvildave/spotify-charts\n",
        "\n",
        "## Group: 2Big2Handle\n",
        "\n",
        "### Group Members\n",
        "\n",
        "| Name                                     | Matrix Number | Task |\n",
        "| :---------------------------------------- | :-------------: | ------------- |\n",
        "| MIKHAIL BIN YASSIN  | A21EC0054  |  \n",
        "| IKMAL BIN KHAIRULEZUAN | A21EC0186 |  \n"
      ],
      "metadata": {
        "id": "6H5zu19c_8fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. About the Dataset\n",
        "\n",
        "The Spotify dataset is about a complete dataset of all the \"Top 200\" and \"Viral 50\" charts published globally by Spotify. Spotify publishes a new chart every 2-3 days. This is its entire collection since January 1, 2017. The dataset consist of 9 columns which are title, rank, date, artist, url, region, chart, trend, and streams. The size of the dataset is 3.48 GB. For this dataset, we will import it from kaggle website rather than download it and import it back to our google drive that will consume our laptop memory storage."
      ],
      "metadata": {
        "id": "752rLKtzAAFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Ip8ThaLOx5z7",
        "outputId": "80d31842-4938-4c8f-dc77-68d12fb0a213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-afa06c9e-dafa-41d8-9732-459057255c87\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-afa06c9e-dafa-41d8-9732-459057255c87\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mikhaily\",\"key\":\"e566853c8e415ee9a7094aa240eeb156\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlMH24tPuuYN"
      },
      "outputs": [],
      "source": [
        "# Create a kaggle folder\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "_r7OW09qxtpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "34TdDc7WxvS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download dhruvildave/spotify-charts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj5XHN3Hxwt0",
        "outputId": "36abb840-304e-4c78-ce80-ca2607ddb781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading spotify-charts.zip to /content\n",
            " 98% 929M/945M [00:10<00:00, 156MB/s]\n",
            "100% 945M/945M [00:10<00:00, 90.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip spotify-charts.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxeje9B7xzLk",
        "outputId": "e6bcd57f-ce3a-4278-cc20-f16230f44495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  spotify-charts.zip\n",
            "  inflating: charts.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note:"
      ],
      "metadata": {
        "id": "39LQFJC0-CNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Functions used to calculate the Computational Metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaLaICJfB-yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using time(), psutil.cpu_percent() and memory_usage() as the main function to calculate the computional metrics in this task.\n",
        "\n",
        "- `time()`: The `time()` function in Python is used to measure the execution time of a piece of code. We will use this function to measure the time taken to execute a specific block of code.\n",
        "\n",
        "- `psutil.cpu_percent()`: The `cpu_percent()` function in the `psutil` library is used to monitor CPU usage in Python. It returns the current system-wide CPU utilization in the form of a percentage. It requires a time interval as a parameter (e.g. seconds). We have include a time interval because CPU use is calculated over a period of time.\n",
        "\n",
        "- `memory_usage()`: The `memory_usage(`) function in pandas is used to calculate the memory usage of a dataframe. It returns the memory usage of each column in the dataframe, as well as the total memory usage of the dataframe. By default, it only considers the memory usage of the data itself, not the memory usage of the index or other metadata. However, you can include the memory usage of the index and metadata by setting the deep parameter to True."
      ],
      "metadata": {
        "id": "hRhORKfLCId_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##`memory_usage()` VS `info()`"
      ],
      "metadata": {
        "id": "1H5OhsroCDPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `memory_usage()` method and the `.info()` method in pandas can be used to check the memory usage of a dataframe, but they provide different levels of detail.\n",
        "\n",
        "The `memory_usage()` method returns the memory usage of each column in the dataframe, as well as the total memory usage of the dataframe. By default, it only considers the memory usage of the data itself, not the memory usage of the index or other metadata. However, you can include the memory usage of the index and metadata by setting the deep parameter to True.\n",
        "\n",
        "On the other hand, the `info()` method provides more detailed information about the dataframe, including the number of rows and columns, the data types of each column, and the memory usage of the dataframe. It also includes information about the index, such as the number of non-null values and the memory usage.\n",
        "\n",
        "The difference between the memory usage reported by `memory_usage()` and `info()` is that `memory_usage()` only reports the memory usage of the data itself, while `info()` reports the memory usage of the data as well as the index and other metadata. Therefore, the memory usage reported by `info()` is generally higher than the memory usage reported by `memory_usage()`."
      ],
      "metadata": {
        "id": "9T_nh5d4CLnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup:"
      ],
      "metadata": {
        "id": "tZF_Z-KpEq2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Import time module\n",
        "import time\n",
        "\n",
        "# Import psutil\n",
        "import psutil"
      ],
      "metadata": {
        "id": "Q9MHhsgOFGSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the results\n",
        "results = {'Tradisional Way':[], 'Load Less Data': [], 'Use Chunking': [], 'Optimize Data Types': [], 'Sampling': [], 'Dask': []}"
      ],
      "metadata": {
        "id": "qBmCJGqtEt82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will print the total memory usage of the dataframe in bytes. If you want to convert it to a more human-readable format, you can use the following function:"
      ],
      "metadata": {
        "id": "kbYDLSeKH-mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_memory_usage(num, suffix='B'):\n",
        "    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return f\"{num:.1f} {unit}{suffix}\"\n",
        "        num /= 1024.0\n",
        "    return f\"{num:.1f} Yi{suffix}\""
      ],
      "metadata": {
        "id": "YyB_AnclH5pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_elapsed_time(elapsed_time):\n",
        "    minutes, seconds = divmod(elapsed_time, 60)\n",
        "    return f'{int(minutes):02d}:{int(seconds):02d}'"
      ],
      "metadata": {
        "id": "3alSkKDzLc_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cpu_usage(cpu_usage):\n",
        "    return f'{cpu_usage:.2f}%'"
      ],
      "metadata": {
        "id": "5hqZArMKMU8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Tradisional Way of Reading Big Data:"
      ],
      "metadata": {
        "id": "lPfK8-DnzC7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with large amounts of data, we must be caution in how we use memory. When we have a large amount of data, memory shortage is a common problem. If all RAM space is consumed, the program will crash and throw a Memory Error, which can be difficult to handle at times. In this case, limiting memory usage becomes critical. Here we show you five smart strategies to handle large datasets effectively with the steps:"
      ],
      "metadata": {
        "id": "pZrBP4m1nte5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Reading the data\n",
        "df = pd.read_csv(\"charts.csv\")\n",
        "\n",
        "# Calculate the CPU usage\n",
        "cpu_usage = format_cpu_usage(psutil.cpu_percent(interval=1))\n",
        "\n",
        "# Calculate the memory usage\n",
        "memory_usage = format_memory_usage(df.memory_usage(deep=True).sum())\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = format_elapsed_time(time.time() - start_time)\n",
        "\n",
        "# Add the result to the dictionary\n",
        "results['Tradisional Way'].append((memory_usage, cpu_usage, elapsed_time))\n",
        "\n",
        "# print the difference between start\n",
        "# and end time in seconds\n",
        "print(results['Tradisional Way'])"
      ],
      "metadata": {
        "id": "9a2mqUXtymKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfedbed-4192-45e0-ed14-ac20881943bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('13.1 GB', '2.00%', '01:47')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "AVlfBDKID_Ko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "a9ffc612-66f8-439c-99c6-b2ea72b8b7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  title  rank        date  \\\n",
              "0               Chantaje (feat. Maluma)     1  2017-01-01   \n",
              "1           Vente Pa' Ca (feat. Maluma)     2  2017-01-01   \n",
              "2            Reggaetón Lento (Bailemos)     3  2017-01-01   \n",
              "3                                Safari     4  2017-01-01   \n",
              "4                           Shaky Shaky     5  2017-01-01   \n",
              "...                                 ...   ...         ...   \n",
              "26173509                            BYE    46  2021-07-31   \n",
              "26173510                        Pillars    47  2021-07-31   \n",
              "26173511                   Gái Độc Thân    48  2021-07-31   \n",
              "26173512  Renegade (feat. Taylor Swift)    49  2021-07-31   \n",
              "26173513                Letter to Jarad    50  2021-07-31   \n",
              "\n",
              "                                         artist  \\\n",
              "0                                       Shakira   \n",
              "1                                  Ricky Martin   \n",
              "2                                          CNCO   \n",
              "3         J Balvin, Pharrell Williams, BIA, Sky   \n",
              "4                                  Daddy Yankee   \n",
              "...                                         ...   \n",
              "26173509                                  Jaden   \n",
              "26173510                                 My Anh   \n",
              "26173511                                  Tlinh   \n",
              "26173512                        Big Red Machine   \n",
              "26173513              LRN Slime, Shiloh Dynasty   \n",
              "\n",
              "                                                        url     region  \\\n",
              "0         https://open.spotify.com/track/6mICuAdrwEjh6Y6...  Argentina   \n",
              "1         https://open.spotify.com/track/7DM4BPaS7uofFul...  Argentina   \n",
              "2         https://open.spotify.com/track/3AEZUABDXNtecAO...  Argentina   \n",
              "3         https://open.spotify.com/track/6rQSrBHf7HlZjtc...  Argentina   \n",
              "4         https://open.spotify.com/track/58IL315gMSTD37D...  Argentina   \n",
              "...                                                     ...        ...   \n",
              "26173509  https://open.spotify.com/track/3OUyyDN7EZrL7i0...    Vietnam   \n",
              "26173510  https://open.spotify.com/track/6eky30oFiQbHUAT...    Vietnam   \n",
              "26173511  https://open.spotify.com/track/2klsSb2iTfgDh95...    Vietnam   \n",
              "26173512  https://open.spotify.com/track/1aU1wpYBSpP0M6I...    Vietnam   \n",
              "26173513  https://open.spotify.com/track/508QhA2SncMbh5C...    Vietnam   \n",
              "\n",
              "            chart          trend   streams  \n",
              "0          top200  SAME_POSITION  253019.0  \n",
              "1          top200        MOVE_UP  223988.0  \n",
              "2          top200      MOVE_DOWN  210943.0  \n",
              "3          top200  SAME_POSITION  173865.0  \n",
              "4          top200        MOVE_UP  153956.0  \n",
              "...           ...            ...       ...  \n",
              "26173509  viral50        MOVE_UP       NaN  \n",
              "26173510  viral50      NEW_ENTRY       NaN  \n",
              "26173511  viral50      MOVE_DOWN       NaN  \n",
              "26173512  viral50      MOVE_DOWN       NaN  \n",
              "26173513  viral50      MOVE_DOWN       NaN  \n",
              "\n",
              "[26173514 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5cce57c7-8905-4a76-b814-1055c5397670\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>rank</th>\n",
              "      <th>date</th>\n",
              "      <th>artist</th>\n",
              "      <th>url</th>\n",
              "      <th>region</th>\n",
              "      <th>chart</th>\n",
              "      <th>trend</th>\n",
              "      <th>streams</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chantaje (feat. Maluma)</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>Shakira</td>\n",
              "      <td>https://open.spotify.com/track/6mICuAdrwEjh6Y6...</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>top200</td>\n",
              "      <td>SAME_POSITION</td>\n",
              "      <td>253019.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vente Pa' Ca (feat. Maluma)</td>\n",
              "      <td>2</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>Ricky Martin</td>\n",
              "      <td>https://open.spotify.com/track/7DM4BPaS7uofFul...</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>top200</td>\n",
              "      <td>MOVE_UP</td>\n",
              "      <td>223988.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reggaetón Lento (Bailemos)</td>\n",
              "      <td>3</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>CNCO</td>\n",
              "      <td>https://open.spotify.com/track/3AEZUABDXNtecAO...</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>top200</td>\n",
              "      <td>MOVE_DOWN</td>\n",
              "      <td>210943.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Safari</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>J Balvin, Pharrell Williams, BIA, Sky</td>\n",
              "      <td>https://open.spotify.com/track/6rQSrBHf7HlZjtc...</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>top200</td>\n",
              "      <td>SAME_POSITION</td>\n",
              "      <td>173865.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Shaky Shaky</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>Daddy Yankee</td>\n",
              "      <td>https://open.spotify.com/track/58IL315gMSTD37D...</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>top200</td>\n",
              "      <td>MOVE_UP</td>\n",
              "      <td>153956.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26173509</th>\n",
              "      <td>BYE</td>\n",
              "      <td>46</td>\n",
              "      <td>2021-07-31</td>\n",
              "      <td>Jaden</td>\n",
              "      <td>https://open.spotify.com/track/3OUyyDN7EZrL7i0...</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>viral50</td>\n",
              "      <td>MOVE_UP</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26173510</th>\n",
              "      <td>Pillars</td>\n",
              "      <td>47</td>\n",
              "      <td>2021-07-31</td>\n",
              "      <td>My Anh</td>\n",
              "      <td>https://open.spotify.com/track/6eky30oFiQbHUAT...</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>viral50</td>\n",
              "      <td>NEW_ENTRY</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26173511</th>\n",
              "      <td>Gái Độc Thân</td>\n",
              "      <td>48</td>\n",
              "      <td>2021-07-31</td>\n",
              "      <td>Tlinh</td>\n",
              "      <td>https://open.spotify.com/track/2klsSb2iTfgDh95...</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>viral50</td>\n",
              "      <td>MOVE_DOWN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26173512</th>\n",
              "      <td>Renegade (feat. Taylor Swift)</td>\n",
              "      <td>49</td>\n",
              "      <td>2021-07-31</td>\n",
              "      <td>Big Red Machine</td>\n",
              "      <td>https://open.spotify.com/track/1aU1wpYBSpP0M6I...</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>viral50</td>\n",
              "      <td>MOVE_DOWN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26173513</th>\n",
              "      <td>Letter to Jarad</td>\n",
              "      <td>50</td>\n",
              "      <td>2021-07-31</td>\n",
              "      <td>LRN Slime, Shiloh Dynasty</td>\n",
              "      <td>https://open.spotify.com/track/508QhA2SncMbh5C...</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>viral50</td>\n",
              "      <td>MOVE_DOWN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26173514 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cce57c7-8905-4a76-b814-1055c5397670')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5cce57c7-8905-4a76-b814-1055c5397670 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5cce57c7-8905-4a76-b814-1055c5397670');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d06a11eb-b084-4a3b-8408-ec5174d09c54\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d06a11eb-b084-4a3b-8408-ec5174d09c54')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d06a11eb-b084-4a3b-8408-ec5174d09c54 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that by using traditional way to read the csv is 1 minute and 47 seconds. Sometime when we want to try load the spotify chart, it show the session crashed after using all available RAM in google collab. To solve this issues, we need to use the high-RAM runtimes by purchases the Colab Pro which cost USD $9.99 per month that is quiet expensive. We need to solve this issues by performing stragies to handle the big data."
      ],
      "metadata": {
        "id": "GDB7HWv0zWO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Strategies for Big Datasets with Steps for Using These Strategies\n",
        "\n"
      ],
      "metadata": {
        "id": "_SbbyPI6LntX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###a. Use Chunking\n",
        "\n",
        "Chunking is the process of dividing a large dataset into smaller pieces based on our preferences. This method is useful when dealing with large datasets that may not fit entirely in memory because when we want to load big data, it will cause crash and memory error. By dividing the data into smaller, more manageable chunks, we gain not only the ability to perform parallel and distributed processing but also the flexibility to perform computations on subsets of the data, allowing for incremental and scalable analyses. Sometimes problems don't fit in the single code, or the RAM could not hold the long execution of code, sometimes dask.arrays or dask.dataframe fails to manage the long Datasets. In this example, we will chunk the dataset into smaller size."
      ],
      "metadata": {
        "id": "hzpEPIMzEAxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the CPU usage monitor\n",
        "cpu_usage = psutil.cpu_percent(interval=1)\n",
        "\n",
        "#----------------------------------\n",
        "file_path = \"charts.csv\"  # Provide the correct file path\n",
        "\n",
        "# Chunk size\n",
        "chunk_size = 2617352  # Adjust this value based on your preferences\n",
        "\n",
        "# Read the CSV file in chunks\n",
        "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
        "    # Process each chunk as needed\n",
        "    # For example, you can print the shape of each chunk\n",
        "    print(f\"Chunk {i+1} Shape: {chunk.shape}\")\n",
        "\n",
        "    # Perform additional processing on each chunk if necessary\n",
        "\n",
        "    # Save the chunk to a separate CSV file\n",
        "    chunk.to_csv(f\"{file_path}_chunk_{i+1}.csv\", index=False)\n",
        "\n",
        "    # Get memory usage of the current chunk\n",
        "    memory_usage = chunk.memory_usage(index=True, deep=False)\n",
        "\n",
        "    # Add the result to the dictionary or do something with memory_usage\n",
        "\n",
        "#----------------------------------\n",
        "\n",
        "# Stop the CPU usage monitor\n",
        "cpu_usage = format_cpu_usage(psutil.cpu_percent(interval=None))\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = format_elapsed_time(time.time() - start_time)\n",
        "\n",
        "# Get full memory_usage after loading less data (if needed)\n",
        "# memory_usage = file_path.memory_usage(index=True, deep=False)\n",
        "\n",
        "# Add the result to the dictionary\n",
        "results['Use Chunking'].append((memory_usage, cpu_usage, elapsed_time))\n",
        "\n",
        "# Print result\n",
        "print(results['Use Chunking'])\n"
      ],
      "metadata": {
        "id": "Nx3w8RsMEKNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453a6a94-4621-45dc-80c2-4c82c425574f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 Shape: (2617352, 9)\n",
            "Chunk 2 Shape: (2617352, 9)\n",
            "Chunk 3 Shape: (2617352, 9)\n",
            "Chunk 4 Shape: (2617352, 9)\n",
            "Chunk 5 Shape: (2617352, 9)\n",
            "Chunk 6 Shape: (2617352, 9)\n",
            "Chunk 7 Shape: (2617352, 9)\n",
            "Chunk 8 Shape: (2617352, 9)\n",
            "Chunk 9 Shape: (2617352, 9)\n",
            "Chunk 10 Shape: (2617346, 9)\n",
            "[(Index           132\n",
            "title      20938768\n",
            "rank       20938768\n",
            "date       20938768\n",
            "artist     20938768\n",
            "url        20938768\n",
            "region     20938768\n",
            "chart      20938768\n",
            "trend      20938768\n",
            "streams    20938768\n",
            "dtype: int64, '66.60%', '04:17')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information about the chunk\n",
        "chunk_info = f\"Chunk {i+1} Info:\\n{chunk.info()}\\n\"\n",
        "\n",
        "# Print or log the chunk information\n",
        "print(chunk_info)"
      ],
      "metadata": {
        "id": "6R0_6jDuEO7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0be72e-dd54-452a-f32b-5348d27cbed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2617346 entries, 23556168 to 26173513\n",
            "Data columns (total 9 columns):\n",
            " #   Column   Dtype  \n",
            "---  ------   -----  \n",
            " 0   title    object \n",
            " 1   rank     int64  \n",
            " 2   date     object \n",
            " 3   artist   object \n",
            " 4   url      object \n",
            " 5   region   object \n",
            " 6   chart    object \n",
            " 7   trend    object \n",
            " 8   streams  float64\n",
            "dtypes: float64(1), int64(1), object(7)\n",
            "memory usage: 179.7+ MB\n",
            "Chunk 10 Info:\n",
            "None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file in chunks\n",
        "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
        "\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "\n",
        "    # Display basic statistics for numerical columns\n",
        "    print(chunk.describe())\n",
        "\n",
        "    # Display the first few rows of the chunk\n",
        "    print(chunk.head())\n",
        "\n",
        "    # Display the data types of columns\n",
        "    print(chunk.dtypes)"
      ],
      "metadata": {
        "id": "v33uZoymEPej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc142d3-65ef-4910-b7e9-54cdb5332068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "               rank       streams\n",
            "count  2.617352e+06  2.450701e+06\n",
            "mean   9.060635e+01  5.282585e+04\n",
            "std    5.822597e+01  2.030654e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    3.900000e+01  3.299000e+03\n",
            "50%    8.500000e+01  9.182000e+03\n",
            "75%    1.400000e+02  3.131100e+04\n",
            "max    2.000000e+02  8.291491e+06\n",
            "                         title  rank        date  \\\n",
            "0      Chantaje (feat. Maluma)     1  2017-01-01   \n",
            "1  Vente Pa' Ca (feat. Maluma)     2  2017-01-01   \n",
            "2   Reggaetón Lento (Bailemos)     3  2017-01-01   \n",
            "3                       Safari     4  2017-01-01   \n",
            "4                  Shaky Shaky     5  2017-01-01   \n",
            "\n",
            "                                  artist  \\\n",
            "0                                Shakira   \n",
            "1                           Ricky Martin   \n",
            "2                                   CNCO   \n",
            "3  J Balvin, Pharrell Williams, BIA, Sky   \n",
            "4                           Daddy Yankee   \n",
            "\n",
            "                                                 url     region   chart  \\\n",
            "0  https://open.spotify.com/track/6mICuAdrwEjh6Y6...  Argentina  top200   \n",
            "1  https://open.spotify.com/track/7DM4BPaS7uofFul...  Argentina  top200   \n",
            "2  https://open.spotify.com/track/3AEZUABDXNtecAO...  Argentina  top200   \n",
            "3  https://open.spotify.com/track/6rQSrBHf7HlZjtc...  Argentina  top200   \n",
            "4  https://open.spotify.com/track/58IL315gMSTD37D...  Argentina  top200   \n",
            "\n",
            "           trend   streams  \n",
            "0  SAME_POSITION  253019.0  \n",
            "1        MOVE_UP  223988.0  \n",
            "2      MOVE_DOWN  210943.0  \n",
            "3  SAME_POSITION  173865.0  \n",
            "4        MOVE_UP  153956.0  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n",
            "Chunk 2:\n",
            "               rank       streams\n",
            "count  2.617352e+06  2.617352e+06\n",
            "mean   9.657586e+01  5.498171e+04\n",
            "std    5.744445e+01  2.089881e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    4.700000e+01  3.207000e+03\n",
            "50%    9.400000e+01  9.225000e+03\n",
            "75%    1.460000e+02  3.384200e+04\n",
            "max    2.000000e+02  1.097739e+07\n",
            "                                       title  rank        date  \\\n",
            "2617352                                   人質   132  2018-06-18   \n",
            "2617353                                   買榜   133  2018-06-18   \n",
            "2617354  Perfect Duet (Ed Sheeran & Beyoncé)   134  2018-06-18   \n",
            "2617355                                  拋物線   135  2018-06-18   \n",
            "2617356                             Delicate   136  2018-06-18   \n",
            "\n",
            "                     artist  \\\n",
            "2617352         A-Mei Chang   \n",
            "2617353  Kumachan, Julia Wu   \n",
            "2617354          Ed Sheeran   \n",
            "2617355          Tanya Chua   \n",
            "2617356        Taylor Swift   \n",
            "\n",
            "                                                       url  region   chart  \\\n",
            "2617352  https://open.spotify.com/track/6OAUhxFZzaRHHC8...  Taiwan  top200   \n",
            "2617353  https://open.spotify.com/track/67LTN04r77CavIk...  Taiwan  top200   \n",
            "2617354  https://open.spotify.com/track/1bhUWB0zJMIKr9y...  Taiwan  top200   \n",
            "2617355  https://open.spotify.com/track/2oZJWtQZ6CLPHHs...  Taiwan  top200   \n",
            "2617356  https://open.spotify.com/track/7CneiDesUKFJO3I...  Taiwan  top200   \n",
            "\n",
            "             trend  streams  \n",
            "2617352  MOVE_DOWN     3396  \n",
            "2617353    MOVE_UP     3395  \n",
            "2617354    MOVE_UP     3380  \n",
            "2617355    MOVE_UP     3378  \n",
            "2617356    MOVE_UP     3370  \n",
            "title      object\n",
            "rank        int64\n",
            "date       object\n",
            "artist     object\n",
            "url        object\n",
            "region     object\n",
            "chart      object\n",
            "trend      object\n",
            "streams     int64\n",
            "dtype: object\n",
            "Chunk 3:\n",
            "               rank       streams\n",
            "count  2.617352e+06  2.617352e+06\n",
            "mean   9.753312e+01  5.482981e+04\n",
            "std    5.768683e+01  2.020506e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    4.700000e+01  3.251000e+03\n",
            "50%    9.600000e+01  9.490000e+03\n",
            "75%    1.470000e+02  3.501500e+04\n",
            "max    2.000000e+02  1.081901e+07\n",
            "                             title  rank        date         artist  \\\n",
            "5234704  High On Life (feat. Bonn)   182  2019-02-22  Martin Garrix   \n",
            "5234705                 fake smile   183  2019-02-22  Ariana Grande   \n",
            "5234706  Space Cadet (feat. Gunna)   184  2019-02-22   Metro Boomin   \n",
            "5234707                       BAD!   185  2019-02-22   XXXTENTACION   \n",
            "5234708                        SOS   186  2019-02-22          5GANG   \n",
            "\n",
            "                                                       url   region   chart  \\\n",
            "5234704  https://open.spotify.com/track/4ut5G4rgB1ClpMT...  Romania  top200   \n",
            "5234705  https://open.spotify.com/track/3wFLWP0FcIqHK1w...  Romania  top200   \n",
            "5234706  https://open.spotify.com/track/1fewSx2d5KIZ04w...  Romania  top200   \n",
            "5234707  https://open.spotify.com/track/22An65gG31JLf9n...  Romania  top200   \n",
            "5234708  https://open.spotify.com/track/2o6zL2P1HRhVwvY...  Romania  top200   \n",
            "\n",
            "             trend  streams  \n",
            "5234704  MOVE_DOWN     1621  \n",
            "5234705  MOVE_DOWN     1583  \n",
            "5234706  MOVE_DOWN     1582  \n",
            "5234707  MOVE_DOWN     1579  \n",
            "5234708  NEW_ENTRY     1572  \n",
            "title      object\n",
            "rank        int64\n",
            "date       object\n",
            "artist     object\n",
            "url        object\n",
            "region     object\n",
            "chart      object\n",
            "trend      object\n",
            "streams     int64\n",
            "dtype: object\n",
            "Chunk 4:\n",
            "               rank       streams\n",
            "count  2.617352e+06  2.617352e+06\n",
            "mean   9.665736e+01  5.639176e+04\n",
            "std    5.775099e+01  2.127557e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    4.600000e+01  3.503000e+03\n",
            "50%    9.400000e+01  9.941000e+03\n",
            "75%    1.460000e+02  3.672100e+04\n",
            "max    2.000000e+02  1.202899e+07\n",
            "                                                     title  rank        date  \\\n",
            "7852056  Sola (Remix) [feat. Daddy Yankee, Wisin, Farru...    84  2019-09-21   \n",
            "7852057                                            Happier    85  2019-09-21   \n",
            "7852058                                             Bonita    86  2019-09-21   \n",
            "7852059               Nena Maldición (feat. Lenny Tavárez)    87  2019-09-21   \n",
            "7852060                                             Créeme    88  2019-09-21   \n",
            "\n",
            "                              artist  \\\n",
            "7852056                     Anuel AA   \n",
            "7852057         Marshmello, Bastille   \n",
            "7852058      Juanes, Sebastian Yatra   \n",
            "7852059  Lenny Tavárez, Paulo Londra   \n",
            "7852060              KAROL G, Maluma   \n",
            "\n",
            "                                                       url       region  \\\n",
            "7852056  https://open.spotify.com/track/5q2JbCNi4Fcnglg...  El Salvador   \n",
            "7852057  https://open.spotify.com/track/2dpaYNEQHiRxtZb...  El Salvador   \n",
            "7852058  https://open.spotify.com/track/0gcOnIUKIG6JF56...  El Salvador   \n",
            "7852059  https://open.spotify.com/track/77dMSg2VHi2wwXj...  El Salvador   \n",
            "7852060  https://open.spotify.com/track/4EKZsrsCKyqr64F...  El Salvador   \n",
            "\n",
            "          chart      trend  streams  \n",
            "7852056  top200  MOVE_DOWN     2196  \n",
            "7852057  top200    MOVE_UP     2144  \n",
            "7852058  top200    MOVE_UP     2132  \n",
            "7852059  top200    MOVE_UP     2106  \n",
            "7852060  top200  MOVE_DOWN     2102  \n",
            "title      object\n",
            "rank        int64\n",
            "date       object\n",
            "artist     object\n",
            "url        object\n",
            "region     object\n",
            "chart      object\n",
            "trend      object\n",
            "streams     int64\n",
            "dtype: object\n",
            "Chunk 5:\n",
            "               rank       streams\n",
            "count  2.617352e+06  1.704891e+06\n",
            "mean   7.043810e+01  5.368763e+04\n",
            "std    5.754513e+01  2.128162e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    2.400000e+01  3.310000e+03\n",
            "50%    4.700000e+01  9.157000e+03\n",
            "75%    1.150000e+02  3.129800e+04\n",
            "max    2.000000e+02  1.138152e+07\n",
            "                                          title  rank        date  \\\n",
            "10469408                           Shape of You   159  2020-03-21   \n",
            "10469409  Loco Contigo (feat. J. Balvin & Tyga)   160  2020-03-21   \n",
            "10469410    Bohemian Rhapsody - Remastered 2011   161  2020-03-21   \n",
            "10469411                        Cuerpo en Venta   162  2020-03-21   \n",
            "10469412                               Verte Ir   163  2020-03-21   \n",
            "\n",
            "                                                     artist  \\\n",
            "10469408                                         Ed Sheeran   \n",
            "10469409                                 DJ Snake, J Balvin   \n",
            "10469410                                              Queen   \n",
            "10469411      Noriel, Myke Towers, Rauw Alejandro, Almighty   \n",
            "10469412  DJ Luian, Mambo Kingz, Anuel AA, Nicky Jam, Da...   \n",
            "\n",
            "                                                        url    region   chart  \\\n",
            "10469408  https://open.spotify.com/track/7qiZfU4dY1lWllz...  Colombia  top200   \n",
            "10469409  https://open.spotify.com/track/6osaMSJh9NguagE...  Colombia  top200   \n",
            "10469410  https://open.spotify.com/track/4u7EnebtmKWzUH4...  Colombia  top200   \n",
            "10469411  https://open.spotify.com/track/2B94bbI1a4LyBpK...  Colombia  top200   \n",
            "10469412  https://open.spotify.com/track/4lzxJ4jCuFDXXGk...  Colombia  top200   \n",
            "\n",
            "              trend  streams  \n",
            "10469408    MOVE_UP   8211.0  \n",
            "10469409  MOVE_DOWN   8170.0  \n",
            "10469410    MOVE_UP   8155.0  \n",
            "10469411  MOVE_DOWN   8110.0  \n",
            "10469412    MOVE_UP   8069.0  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n",
            "Chunk 6:\n",
            "               rank       streams\n",
            "count  2.617352e+06  1.043350e+06\n",
            "mean   5.426352e+01  5.472261e+04\n",
            "std    5.186878e+01  2.038630e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    1.800000e+01  3.793000e+03\n",
            "50%    3.600000e+01  9.692000e+03\n",
            "75%    7.100000e+01  3.707500e+04\n",
            "max    2.000000e+02  7.979485e+06\n",
            "                                title  rank        date  \\\n",
            "13086760              Te Boté - Remix     4  2018-06-15   \n",
            "13086761  Y Dale Alegría a Mi Corazón     5  2018-06-15   \n",
            "13086762                   Sin Pijama     6  2018-06-15   \n",
            "13086763                    Pa Dentro     7  2018-06-15   \n",
            "13086764               United By Love     8  2018-06-15   \n",
            "\n",
            "                                                     artist  \\\n",
            "13086760  Nio Garcia, Casper Magico, Bad Bunny, Darell, ...   \n",
            "13086761                                               Lali   \n",
            "13086762                             Becky G, Natti Natasha   \n",
            "13086763                                             Juanes   \n",
            "13086764                                     Natalia Oreiro   \n",
            "\n",
            "                                                        url   region    chart  \\\n",
            "13086760  https://open.spotify.com/track/3V8UKqhEK5zBkBb...  Uruguay  viral50   \n",
            "13086761  https://open.spotify.com/track/4LI3XE2CwsxrKdz...  Uruguay  viral50   \n",
            "13086762  https://open.spotify.com/track/2ijef6ni2amuunR...  Uruguay  viral50   \n",
            "13086763  https://open.spotify.com/track/7flNdUykV5LYDRv...  Uruguay  viral50   \n",
            "13086764  https://open.spotify.com/track/2tfZmwWj1joy1iC...  Uruguay  viral50   \n",
            "\n",
            "                  trend  streams  \n",
            "13086760        MOVE_UP      NaN  \n",
            "13086761      MOVE_DOWN      NaN  \n",
            "13086762  SAME_POSITION      NaN  \n",
            "13086763  SAME_POSITION      NaN  \n",
            "13086764        MOVE_UP      NaN  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n",
            "Chunk 7:\n",
            "               rank       streams\n",
            "count  2.617352e+06  2.617351e+06\n",
            "mean   9.826043e+01  5.524079e+04\n",
            "std    5.750791e+01  2.086971e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    4.800000e+01  3.904000e+03\n",
            "50%    9.700000e+01  9.855000e+03\n",
            "75%    1.480000e+02  3.793800e+04\n",
            "max    2.000000e+02  1.722324e+07\n",
            "                                          title  rank        date  \\\n",
            "15704112                            Gang În Sus    82  2020-11-05   \n",
            "15704113                                   Nein    83  2020-11-05   \n",
            "15704114           Mood Swings (feat. Lil Tjay)    84  2020-11-05   \n",
            "15704115  The Woo (feat. 50 Cent & Roddy Ricch)    85  2020-11-05   \n",
            "15704116              Spicy (feat. Post Malone)    86  2020-11-05   \n",
            "\n",
            "                         artist  \\\n",
            "15704112  Nane, Oscar, Super Ed   \n",
            "15704113                   Nane   \n",
            "15704114              Pop Smoke   \n",
            "15704115              Pop Smoke   \n",
            "15704116          Ty Dolla $ign   \n",
            "\n",
            "                                                        url   region   chart  \\\n",
            "15704112  https://open.spotify.com/track/131QzZD2qg9fE0n...  Romania  top200   \n",
            "15704113  https://open.spotify.com/track/64aUDGfRGc52k2f...  Romania  top200   \n",
            "15704114  https://open.spotify.com/track/5rZlwNFl01HqLWB...  Romania  top200   \n",
            "15704115  https://open.spotify.com/track/1H7KnK26kc1Yyel...  Romania  top200   \n",
            "15704116  https://open.spotify.com/track/5IUtvfNvOyVYZUa...  Romania  top200   \n",
            "\n",
            "                  trend  streams  \n",
            "15704112  SAME_POSITION   3312.0  \n",
            "15704113      MOVE_DOWN   3304.0  \n",
            "15704114        MOVE_UP   3303.0  \n",
            "15704115        MOVE_UP   3237.0  \n",
            "15704116        MOVE_UP   3233.0  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n",
            "Chunk 8:\n",
            "               rank       streams\n",
            "count  2.617352e+06  1.513719e+06\n",
            "mean   6.768104e+01  5.490283e+04\n",
            "std    5.740697e+01  2.125391e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    2.200000e+01  3.983000e+03\n",
            "50%    4.400000e+01  9.713000e+03\n",
            "75%    1.110000e+02  3.749900e+04\n",
            "max    2.000000e+02  1.258664e+07\n",
            "                                      title  rank        date  \\\n",
            "18321464          Naughty List (with Dixie)   119  2020-11-30   \n",
            "18321465  Christmas (Baby Please Come Home)   120  2020-11-30   \n",
            "18321466                        Money Trees   121  2020-11-30   \n",
            "18321467                Christmas Tree Farm   122  2020-11-30   \n",
            "18321468                everything i wanted   123  2020-11-30   \n",
            "\n",
            "                            artist  \\\n",
            "18321464                Liam Payne   \n",
            "18321465              Mariah Carey   \n",
            "18321466  Kendrick Lamar, Jay Rock   \n",
            "18321467              Taylor Swift   \n",
            "18321468             Billie Eilish   \n",
            "\n",
            "                                                        url  region   chart  \\\n",
            "18321464  https://open.spotify.com/track/2Y0ktCGrGoGcQFX...  Latvia  top200   \n",
            "18321465  https://open.spotify.com/track/3PIDciSFdrQxSQS...  Latvia  top200   \n",
            "18321466  https://open.spotify.com/track/2HbKqm4o0w5wEeE...  Latvia  top200   \n",
            "18321467  https://open.spotify.com/track/2mvabkN1i2gLnGA...  Latvia  top200   \n",
            "18321468  https://open.spotify.com/track/3ZCTVFBt2Brf31R...  Latvia  top200   \n",
            "\n",
            "              trend  streams  \n",
            "18321464  NEW_ENTRY   1091.0  \n",
            "18321465  NEW_ENTRY   1086.0  \n",
            "18321466  NEW_ENTRY   1083.0  \n",
            "18321467  NEW_ENTRY   1083.0  \n",
            "18321468  NEW_ENTRY   1079.0  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n",
            "Chunk 9:\n",
            "               rank       streams\n",
            "count  2.617352e+06  2.224476e+06\n",
            "mean   8.648091e+01  5.877071e+04\n",
            "std    5.908064e+01  2.207723e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    3.400000e+01  3.825000e+03\n",
            "50%    7.800000e+01  9.763000e+03\n",
            "75%    1.370000e+02  3.729500e+04\n",
            "max    2.000000e+02  1.974970e+07\n",
            "                            title  rank        date         artist  \\\n",
            "20938816                 Infinity    39  2020-07-26  One Direction   \n",
            "20938817                   طرشوله    40  2020-07-26        Queen G   \n",
            "20938818           Someone To You    41  2020-07-26        BANNERS   \n",
            "20938819  We Paid (feat. 42 Dugg)    42  2020-07-26       Lil Baby   \n",
            "20938820      no song without you    43  2020-07-26          HONNE   \n",
            "\n",
            "                                                        url  \\\n",
            "20938816  https://open.spotify.com/track/6N5xh0tYYLTQRiC...   \n",
            "20938817  https://open.spotify.com/track/6zKRAoVTjxGaXYx...   \n",
            "20938818  https://open.spotify.com/track/2f0pn9DkEJwAzXA...   \n",
            "20938819  https://open.spotify.com/track/6gxKUmycQX7uyMw...   \n",
            "20938820  https://open.spotify.com/track/1lNHWPDvKEbamKe...   \n",
            "\n",
            "                        region    chart          trend  streams  \n",
            "20938816  United Arab Emirates  viral50        MOVE_UP      NaN  \n",
            "20938817  United Arab Emirates  viral50  SAME_POSITION      NaN  \n",
            "20938818  United Arab Emirates  viral50        MOVE_UP      NaN  \n",
            "20938819  United Arab Emirates  viral50      MOVE_DOWN      NaN  \n",
            "20938820  United Arab Emirates  viral50      NEW_ENTRY      NaN  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n",
            "Chunk 10:\n",
            "               rank       streams\n",
            "count  2.617346e+06  9.153600e+05\n",
            "mean   5.073460e+01  5.625096e+04\n",
            "std    4.980613e+01  2.104789e+05\n",
            "min    1.000000e+00  1.001000e+03\n",
            "25%    1.700000e+01  3.949000e+03\n",
            "50%    3.400000e+01  9.798000e+03\n",
            "75%    5.500000e+01  3.704800e+04\n",
            "max    2.000000e+02  1.062930e+07\n",
            "                           title  rank        date  \\\n",
            "23556168                Face Off   190  2021-11-23   \n",
            "23556169                 Madonna   191  2021-11-23   \n",
            "23556170    lovely (with Khalid)   192  2021-11-23   \n",
            "23556171  Happy End (feat. Sido)   193  2021-11-23   \n",
            "23556172                  Extasy   194  2021-11-23   \n",
            "\n",
            "                                                  artist  \\\n",
            "23556168  Tech N9ne, Joey Cool, King Iso, Dwayne Johnson   \n",
            "23556169                               Bausa, Apache 207   \n",
            "23556170                                   Billie Eilish   \n",
            "23556171                                     Vanessa Mai   \n",
            "23556172         187 Strassenbande, Bonez MC, Frauenarzt   \n",
            "\n",
            "                                                        url   region   chart  \\\n",
            "23556168  https://open.spotify.com/track/6M47gaKejso9772...  Austria  top200   \n",
            "23556169  https://open.spotify.com/track/6a3kNt1UlUOapDK...  Austria  top200   \n",
            "23556170  https://open.spotify.com/track/0u2P5u6lvoDfwTY...  Austria  top200   \n",
            "23556171  https://open.spotify.com/track/1YRaS0rNK8y9E5N...  Austria  top200   \n",
            "23556172  https://open.spotify.com/track/79lc69ZXfL4WmJI...  Austria  top200   \n",
            "\n",
            "              trend  streams  \n",
            "23556168    MOVE_UP   4852.0  \n",
            "23556169    MOVE_UP   4852.0  \n",
            "23556170    MOVE_UP   4835.0  \n",
            "23556171    MOVE_UP   4826.0  \n",
            "23556172  NEW_ENTRY   4814.0  \n",
            "title       object\n",
            "rank         int64\n",
            "date        object\n",
            "artist      object\n",
            "url         object\n",
            "region      object\n",
            "chart       object\n",
            "trend       object\n",
            "streams    float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the chunking method, we can divide the original dataset into several chunks based on a specified chunk size. This method enables us to manage large datasets efficiently. To determine an appropriate chunk size, we can use the formula: number of rows / desired number of chunks. This formula ensures that each chunk is of manageable size while covering the entire dataset. The code example provided reads the CSV file in chunks, processes each chunk as needed, and saves them as separate CSV files. Adjusting the chunk size provides flexibility based on the user's preferences and system capabilities."
      ],
      "metadata": {
        "id": "8ERWApi0EUVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###b. Parallelize with Dask\n",
        "\n",
        "For most BigData analytics, Pandas and NumPy will be used. All of the aforementioned packages support a wide range of computations. However, if the dataset does not fit in memory, these packages will not scale. Dask appears. When a dataset does not \"fit in memory,\" dask expands it to \"fit on disk.\" Depending on the size of the dataset, Dask allows us to easily scale out to clusters or scale down to a single machine. We will see how the dask work in our code below."
      ],
      "metadata": {
        "id": "wy8HwF0HKAEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dask library\n",
        "!pip install dask"
      ],
      "metadata": {
        "id": "Sd4LouYKKFX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the CPU usage monitor\n",
        "cpu_usage = psutil.cpu_percent(interval=1)\n",
        "\n",
        "#----------------------------------\n",
        "# Import dask\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "df_strategies = dd.read_csv(\"charts.csv\")\n",
        "print(df_strategies)\n",
        "#----------------------------------\n",
        "\n",
        "# Stop the CPU usage monitor\n",
        "cpu_usage = format_cpu_usage(psutil.cpu_percent(interval=None))\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = format_elapsed_time(time.time() - start_time)\n",
        "\n",
        "# Get full memory_usage after loading less data\n",
        "memory_usage = df_strategies.memory_usage(index=True, deep=False)\n",
        "\n",
        "# Add the result to the dictionary\n",
        "results['Dask'].append((memory_usage, cpu_usage, elapsed_time))\n",
        "\n",
        "#print result\n",
        "print(results['Dask'])"
      ],
      "metadata": {
        "id": "G0wKkHm3KN1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09623424-330e-4890-bd21-ff5f1a5795d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask DataFrame Structure:\n",
            "                 title   rank    date  artist     url  region   chart   trend streams\n",
            "npartitions=54                                                                       \n",
            "                object  int64  object  object  object  object  object  object   int64\n",
            "                   ...    ...     ...     ...     ...     ...     ...     ...     ...\n",
            "...                ...    ...     ...     ...     ...     ...     ...     ...     ...\n",
            "                   ...    ...     ...     ...     ...     ...     ...     ...     ...\n",
            "                   ...    ...     ...     ...     ...     ...     ...     ...     ...\n",
            "Dask Name: read-csv, 1 graph layer\n",
            "[(Dask Series Structure:\n",
            "npartitions=1\n",
            "    int64\n",
            "      ...\n",
            "dtype: int64\n",
            "Dask Name: series-groupby-sum-agg, 5 graph layers, '39.80%', '00:02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get full memory_usage after loading less data\n",
        "memory_usage = df_strategies.memory_usage(index=True, deep=False)\n",
        "memory_usage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJEfZdcjd6FS",
        "outputId": "34acc3c6-d6ea-46c5-9162-06b56f2a7cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dask Series Structure:\n",
              "npartitions=1\n",
              "    int64\n",
              "      ...\n",
              "dtype: int64\n",
              "Dask Name: series-groupby-sum-agg, 27 graph layers"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###c. Load Less Data\n",
        "\n",
        "With this method, we only load only the essential portions of the dataset to optimize memory usage."
      ],
      "metadata": {
        "id": "QfUEdIaJAO8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the CPU usage monitor\n",
        "cpu_usage = psutil.cpu_percent(interval=1)\n",
        "\n",
        "#----------------------------------\n",
        "# remove unwanted columns in our dataset\n",
        "df_strategies = df_strategies.drop(['url', 'trend'], axis=1)\n",
        "#----------------------------------\n",
        "\n",
        "# Stop the CPU usage monitor\n",
        "cpu_usage = format_cpu_usage(psutil.cpu_percent(interval=None))\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = format_elapsed_time(time.time() - start_time)\n",
        "\n",
        "# Get full memory_usage after loading less data\n",
        "memory_usage = df_strategies.memory_usage(index=True, deep=False)\n",
        "\n",
        "# Add the result to the dictionary\n",
        "results['Load Less Data'].append((memory_usage, cpu_usage, elapsed_time))\n",
        "\n",
        "#print result\n",
        "print(results['Load Less Data'])"
      ],
      "metadata": {
        "id": "XcA6jDxJLVjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed1b70c-58cd-4223-d1d5-695c9db86eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(Dask Series Structure:\n",
            "npartitions=1\n",
            "    int64\n",
            "      ...\n",
            "dtype: int64\n",
            "Dask Name: series-groupby-sum-agg, 6 graph layers, '100.00%', '00:01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the dtype of the dataframe\n",
        "print(df_strategies.dtypes)"
      ],
      "metadata": {
        "id": "bdhlY37YuiH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f90e5de-173b-4334-db09-29c3de1c01e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title      object\n",
            "rank        int64\n",
            "date       object\n",
            "artist     object\n",
            "region     object\n",
            "chart      object\n",
            "streams     int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the info of the dataframe\n",
        "print(df_strategies.info())"
      ],
      "metadata": {
        "id": "Gqc91Kfe1D5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d245103e-0290-4347-f4a9-f324d6e2bf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dask.dataframe.core.DataFrame'>\n",
            "Columns: 7 entries, title to streams\n",
            "dtypes: object(5), int64(2)None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is especially beneficial to implement the \"Load Less Data\" method because it allows the analyst to selectively load and process only the essential portions of the dataset, omitting sections that may not contribute meaningfully to the analysis. We not only save memory resources by avoiding the unnecessary loading of irrelevant data, but we also streamline the analytical workflow by focusing computational efforts on the most relevant information. This method is a successful strategy for optimizing memory usage and increasing the efficiency of data analysis tasks."
      ],
      "metadata": {
        "id": "Q18Oh3wrYPcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###d. Optimize Data Types\n",
        "\n",
        "The Optimizing Data Types method is the process of selecting the most efficient and suitable data types to represent the values in a dataset. This method aims to reduce memory consumption while increasing computational efficiency. By choosing suitable data types that match the range and precision of the actual data, we can reduce storage requirements while improving data processing task performance. To reduce memory usage in Google Colab, we will change the data type column in the code below based on the appropriate data type."
      ],
      "metadata": {
        "id": "kqvE7I2UAWdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_strategies.info()"
      ],
      "metadata": {
        "id": "VUiZOgS_ss4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb852ad8-9cbf-4d06-bf2a-8738ebdf2f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dask.dataframe.core.DataFrame'>\n",
            "Columns: 7 entries, title to streams\n",
            "dtypes: object(5), int64(2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the size of a pandas dataframe, you can use the memory_usage() method. Here’s an example:"
      ],
      "metadata": {
        "id": "kZJCq8YC5jyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the memory usage of the dataframe\n",
        "print(df_strategies.memory_usage(deep=True).sum())\n"
      ],
      "metadata": {
        "id": "3M6Oj-lplfto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483f9ee8-efe3-41c4-cd82-aedaf109bef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dd.Scalar<series-..., dtype=int64>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert each column to the appropriate data type using the astype() and to_numeric() methods. For example, we convert the rank column to an integer data type using the to_numeric() method with the downcast parameter set to 'integer'."
      ],
      "metadata": {
        "id": "0ICd2nxr6Oq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the CPU usage monitor\n",
        "cpu_usage = psutil.cpu_percent(interval=1)\n",
        "\n",
        "#----------------------------------\n",
        "# Convert 'title' to category\n",
        "df_strategies['title'] = df_strategies['title'].astype('category')\n",
        "\n",
        "# Convert 'rank' to numeric with handling of non-numeric values\n",
        "#df_strategies['rank'] = dd.to_numeric(df_strategies['rank'], errors='coerce', downcast='integer')\n",
        "df_strategies['rank'] = df_strategies['rank'].astype('int8')\n",
        "\n",
        "# Convert 'date' to datetime\n",
        "df_strategies['date'] = dd.to_datetime(df_strategies['date'])\n",
        "\n",
        "# Convert 'artist' to category\n",
        "df_strategies['artist'] = df_strategies['artist'].astype('category')\n",
        "\n",
        "# Convert 'region' to category\n",
        "df_strategies['region'] = df_strategies['region'].astype('category')\n",
        "\n",
        "# Convert 'chart' to category\n",
        "df_strategies['chart'] = df_strategies['chart'].astype('category')\n",
        "\n",
        "# Convert 'streams' to numeric\n",
        "#df_strategies['streams'] = pd.to_numeric(df_strategies['streams'], errors='coerce', downcast='float')\n",
        "df_strategies['streams'] = df_strategies['streams'].astype('float')\n",
        "\n",
        "#----------------------------------\n",
        "\n",
        "# Stop the CPU usage monitor\n",
        "cpu_usage = format_cpu_usage(psutil.cpu_percent(interval=None))\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = format_elapsed_time(time.time() - start_time)\n",
        "\n",
        "# Get full memory_usage after loading less data\n",
        "memory_usage = format_memory_usage(df.memory_usage(deep=True).sum())\n",
        "\n",
        "# Add the result to the dictionary\n",
        "results['Optimize Data Types'].append((memory_usage, cpu_usage, elapsed_time))\n",
        "\n",
        "#print result\n",
        "print(results['Optimize Data Types'])"
      ],
      "metadata": {
        "id": "Te9Wuylr6N9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde1cd95-f13b-40f1-86e9-4f7ea93ee0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('13.1 GB', '41.20%', '00:01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###e. Sampling"
      ],
      "metadata": {
        "id": "2oMLdGPnAZ2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling is a useful method for dealing with big data when analyzing the entire dataset is impractical due to its size. Rather than analyzing the entire dataset, sampling involves selecting a representative subset for analysis. Here’s an example of how to use the Sampling strategy in dask:"
      ],
      "metadata": {
        "id": "q_idIfGe61r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Start the CPU usage monitor\n",
        "cpu_usage = psutil.cpu_percent(interval=1)\n",
        "\n",
        "#----------------------------------\n",
        "# Sample 10% of the dataset\n",
        "sampled_df = df_strategies.sample(frac=0.1)\n",
        "#----------------------------------\n",
        "\n",
        "# Stop the CPU usage monitor\n",
        "cpu_usage = format_cpu_usage(psutil.cpu_percent(interval=None))\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = format_elapsed_time(time.time() - start_time)\n",
        "\n",
        "# Get full memory_usage after loading less data\n",
        "memory_usage = format_memory_usage(df.memory_usage(deep=True).sum())\n",
        "\n",
        "# Add the result to the dictionary\n",
        "results['Sampling'].append((memory_usage, cpu_usage, elapsed_time))\n",
        "\n",
        "#print result\n",
        "print(results['Sampling'])"
      ],
      "metadata": {
        "id": "NAKP4l5v6zg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34549237-9b67-4462-a3bf-58c26cc7540d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('13.1 GB', '0.00%', '00:01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the sample() method to randomly select 10% of the rows from the dataset.\n",
        "\n",
        "The frac parameter specifies the fraction of rows to return, which can be a float between 0 and 1. For example, frac=0.1 returns 10% of the rows."
      ],
      "metadata": {
        "id": "Cpm4qyYC65tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Comparative Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "3RGgR18xDwnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Traditional way of Reading Big data\n",
        "\n",
        "| Strategy | CPU Usage (%) | Time Taken (seconds) | Memory Usage (MB) |\n",
        "|-------------------------|-------------------|-----------------------|----------------|\n",
        "| Traditional Method         |      2.00         |      107           |    13100 (13.1GB)      |\n",
        "\n",
        "\n",
        "Strategies for Big Datasets (Dask Integration)\n",
        "\n",
        "| Strategy | CPU Usage (%) | Time Taken (seconds) | Memory Usage (MB) |\n",
        "|------------|-------------------|-----------------------|----------------|\n",
        "|Chunking  | 66.60%   |   257     | 176.7  |\n",
        "|Dask      |     39.80        |    2             |     N/A     |\n",
        "| Load Less Data |        100.00       |     1       |     N/A    |\n",
        "| Optimize Data Types |    41.20     |         1      |  13100   |\n",
        "| Sampling |     0.00    |      1       |     13100     |"
      ],
      "metadata": {
        "id": "RMvGof4unnfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis\n",
        "- **Chunking** takes the longest time to do and require quite alot of CPU usage.\n",
        "\n",
        "- **Dask** stands out as the most efficient strategy in terms of both time taken and memory usage.\n",
        "- `Load Less Data with Dask` is the fastest but uses 100% CPU, which may not be sustainable.\n",
        "- `Optimizing Data Types with Dask` reduces memory usage but doesn’t significantly impact time.\n",
        "- `Sampling with Dask` doesn’t improve resource usage significantly."
      ],
      "metadata": {
        "id": "klMRBArGmpqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Conclusion\n",
        "\n"
      ],
      "metadata": {
        "id": "Ad7g1hZ2AmEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dask:\n",
        "\n",
        "  -   Efficiency: Dask shines when dealing with large datasets. Its parallel and distributed computing capabilities allow it to efficiently process data across multiple cores or even clusters. By breaking down computations into smaller tasks, Dask minimizes bottlenecks and maximizes resource utilization.\n",
        "  - Memory Optimization: Dask intelligently manages memory usage. Unlike traditional methods that load the entire dataset into memory, Dask operates on smaller chunks, reducing the risk of memory exhaustion. It dynamically spills data to disk when needed, ensuring smooth execution even with limited RAM.\n",
        "\n",
        "2. Load Less Data with Dask:\n",
        "  - Speed: This strategy prioritizes speed by loading only the necessary data. However, it comes at the cost of high CPU usage.\n",
        "  - Use Case: When you need rapid insights from a massive dataset and can tolerate short bursts of high CPU load, loading less data with Dask is a pragmatic choice.\n",
        "3.Optimize Data Types with Dask:\n",
        "  - Memory Reduction: Dask allows you to optimize data types (e.g., using int32 instead of int64, or using categorical data). By reducing memory footprint, you gain efficiency without compromising accuracy. This strategy is especially valuable when memory constraints are critical.\n",
        "  - Trade-Off: While it won’t significantly impact processing time, it pays off in memory savings. Consider it a low-hanging fruit for memory optimization.\n",
        "\n",
        "4. Sampling with Dask:\n",
        "  - Quick Insights: Sampling provides a glimpse into the dataset without processing the entire thing. It’s useful for exploratory analysis, hypothesis testing, or initial model building. However, it doesn’t fundamentally change resource usage.\n",
        "  - Limitations: Sampling may miss rare events or outliers, so use it judiciously. For statistical confidence, consider larger samples or other strategies.\n",
        "\n",
        "In summary, Dask’s flexibility, memory management, and scalability make it a powerful tool for big data. Choosing the best strategies need time and multiple trial and error to find what is the most suitable strategies to use and how to use it."
      ],
      "metadata": {
        "id": "HPOXAqCQmuU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "- https://www.geeksforgeeks.org/how-to-check-the-execution-time-of-python-script/\n",
        "- https://www.geeksforgeeks.org/introduction-to-dask-in-python/\n",
        "- https://www.coiled.io/blog/dask-dtype-astype"
      ],
      "metadata": {
        "id": "i_PrHvIaAoMk"
      }
    }
  ]
}